{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ajustes de SO, Java, PySpark e dados (shell + Python)"
      ],
      "metadata": {
        "id": "kCFH-knAO11h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 1 ==========================\n",
        "# Linhas que começam com \"!\" são COMANDOS DE SHELL executados pelo Jupyter\n",
        "# diretamente no sistema operacional (Ubuntu/Debian no Colab/WSL/etc.). Esses\n",
        "# comandos NÃO rodam no interpretador Python — eles chamam binários do SO.\n",
        "\n",
        "# Atualiza o índice de pacotes do apt (lista local de pacotes/versões disponíveis nos repositórios).\n",
        "# Não instala nada; apenas sincroniza metadados para que instalações futuras encontrem as versões mais novas.\n",
        "!apt-get update\n",
        "\n",
        "# Instala o JDK 8 (headless = sem componentes gráficos).\n",
        "# O Apache Spark exige uma JVM. Versões do Spark são compatíveis com Java 8/11/17 (verifique a sua).\n",
        "# O \" -qq > /dev/null \" suprime saídas detalhadas, deixando o output mais limpo.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# ---------- A partir daqui é Python ----------\n",
        "import os  # Módulo padrão do Python para interagir com o Sistema Operacional (variáveis de ambiente, paths, etc.)\n",
        "\n",
        "# Variável de ambiente JAVA_HOME: aponta para o diretório raiz do JDK.\n",
        "# Diversas ferramentas (incluindo o Spark) usam isso para localizar 'java' e 'javac'.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# update-alternatives permite escolher qual versão do 'java' será o padrão do sistema\n",
        "# quando existem múltiplas instalações. Aqui, fixamos a versão instalada acima.\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "\n",
        "# Checa a versão ativa do Java para confirmar que a configuração está OK.\n",
        "!java -version\n",
        "\n",
        "# Instala o cliente PySpark (biblioteca Python que conversa com o engine do Spark).\n",
        "# Sem isso, \"from pyspark import ...\" não funcionaria.\n",
        "!pip install pyspark\n",
        "\n",
        "# Baixa (clona) o repositório com os arquivos de exemplo e os datasets (logs da NASA).\n",
        "# \"git clone <url>\" cria uma pasta local \"aulapython/\" com o conteúdo remoto.\n",
        "!git clone https://github.com/leonardoamorim/aulapython.git\n",
        "\n",
        "# Lista os arquivos dentro de \"aulapython/\" para verificar o clone com sucesso.\n",
        "! ls aulapython/\n",
        "\n",
        "# Descompacta os arquivos de log (.gz) para .txt (sem compressão), assim o Spark e o shell\n",
        "# conseguem ler normalmente. 'gunzip' substitui o arquivo .gz pelo extraído.\n",
        "! gunzip aulapython/NASA_access_log_Jul95.gz\n",
        "! gunzip aulapython/NASA_access_log_Aug95.gz\n",
        "\n",
        "# Mostra o tamanho (uso de disco) de cada arquivo (opção -h = \"human readable\": KB/MB/GB).\n",
        "! du -h aulapython/NASA_access_log_Jul95\n",
        "! du -h aulapython/NASA_access_log_Aug95\n",
        "\n",
        "# Conta quantas linhas existem (cada linha = 1 requisição registrada).\n",
        "! wc -l aulapython/NASA_access_log_Jul95\n",
        "! wc -l aulapython/NASA_access_log_Aug95\n",
        "\n",
        "# Mostra as 10 primeiras linhas do arquivo de Julho para inspecionar o formato:\n",
        "# Formato comum (Common Log Format estendido):\n",
        "# host ident authuser [date:time zone] \"request\" status bytes\n",
        "! head -n10 aulapython/NASA_access_log_Jul95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElVGhgi7O2JR",
        "outputId": "974846fd-d717-4a0a-d8eb-cd6e2267fbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,002 kB]\n",
            "Hit:4 https://cli.github.com/packages stable InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [80.4 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,237 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [43.0 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,789 kB]\n",
            "Fetched 24.4 MB in 5s (4,447 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "openjdk version \"1.8.0_462\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_462-8u462-ga~us1-0ubuntu2~22.04.2-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.462-b08, mixed mode)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Cloning into 'aulapython'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Total 51 (delta 0), reused 0 (delta 0), pack-reused 51 (from 1)\u001b[K\n",
            "Receiving objects: 100% (51/51), 90.32 MiB | 21.71 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            " airlinedelaycauses_DelayedFlights.csv.part01.rar\n",
            " airlinedelaycauses_DelayedFlights.csv.part02.rar\n",
            " airlinedelaycauses_DelayedFlights.csv.part03.rar\n",
            " class_summary_statistics_asyncio.py\n",
            " class_summary_statistics_numba.py\n",
            " class_summary_statistics.py\n",
            " colaboradores.csv\n",
            " colaboradores_data_missing.csv\n",
            " exercicio_01.py\n",
            " funcionarios.json\n",
            "'HPC_com_Python_para_Big_Data_lab_vecAdd_PyCUDA (1).ipynb'\n",
            " IRIS.csv\n",
            " iris.data\n",
            " KNN-RegressaoLogistica.ipynb\n",
            " NASA_access_log_Aug95.gz\n",
            " NASA_access_log_Jul95.gz\n",
            "'Operações avançadas com DataFrame usando PySpark.ipynb'\n",
            "'Operacoes com dados faltantes.ipynb'\n",
            "'Operacoes com Datas e Timestamps.ipynb'\n",
            "'Pandas - Intro.ipynb'\n",
            " programa1.py\n",
            " programa2.py\n",
            " programa3.py\n",
            "'Pyspark - Operações Básicas com DF.ipynb'\n",
            "' Python para Big Data - Introdução a Programação Funcional com Python Funções Lambda em Python.ipynb'\n",
            "'RAPIDS(1).ipynb'\n",
            " Regressão_linear_com_Python.ipynb\n",
            " Regressão_logística_com_Python.ipynb\n",
            " titanic_test.csv\n",
            " titanic_train.csv\n",
            " Tutorial_Cuda_no_Colab.ipynb\n",
            " USA_Housing.csv\n",
            " Visualizacao-de-Dados-Dataset-Iris.ipynb\n",
            "196M\taulapython/NASA_access_log_Jul95\n",
            "161M\taulapython/NASA_access_log_Aug95\n",
            "1891714 aulapython/NASA_access_log_Jul95\n",
            "1569898 aulapython/NASA_access_log_Aug95\n",
            "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
            "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
            "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\n",
            "burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\n",
            "199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179\n",
            "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0\n",
            "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0\n",
            "205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985\n",
            "d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
            "129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Texto explicativo sobre RDD (docstring)"
      ],
      "metadata": {
        "id": "UdLbLsnTO3HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 2 ==========================\n",
        "\"\"\"\n",
        "# Conjunto de dados distribuído resiliente (RDD)\n",
        "\n",
        "O RDD é a API de baixo nível do Spark: uma coleção distribuída e imutável,\n",
        "particionada e processável em paralelo. Você aplica TRANSFORMAÇÕES (lazy),\n",
        "como `map`, `filter`, `reduceByKey`, e executa com AÇÕES como `count`,\n",
        "`collect`, `take`.\n",
        "\n",
        "Quando usar RDDs?\n",
        "- Quando precisa de controle de baixo nível.\n",
        "- Dados não estruturados (logs, texto).\n",
        "- Preferência por programação funcional ao invés de expressões SQL.\n",
        "- Não precisa impor esquema (colunas).\n",
        "- Aceita abrir mão de otimizações automáticas de DataFrames (Catalyst/Tungsten).\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9Vx3YhRfO4kw",
        "outputId": "51bd4a05-b4d1-4769-882d-d06b52c6407d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Conjunto de dados distribuído resiliente (RDD)\\n\\nO RDD é a API de baixo nível do Spark: uma coleção distribuída e imutável,\\nparticionada e processável em paralelo. Você aplica TRANSFORMAÇÕES (lazy),\\ncomo `map`, `filter`, `reduceByKey`, e executa com AÇÕES como `count`,\\n`collect`, `take`.\\n\\nQuando usar RDDs?\\n- Quando precisa de controle de baixo nível.\\n- Dados não estruturados (logs, texto).\\n- Preferência por programação funcional ao invés de expressões SQL.\\n- Não precisa impor esquema (colunas).\\n- Aceita abrir mão de otimizações automáticas de DataFrames (Catalyst/Tungsten).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuração e criação do SparkContext"
      ],
      "metadata": {
        "id": "oPtFhEB0O5ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 3 ==========================\n",
        "# \"from pyspark import SparkConf, SparkContext\"\n",
        "# - SparkConf: objeto de configuração (master, appName, memória, etc).\n",
        "# - SparkContext: ponto de entrada da API de RDD (baixo nível).\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# 'add' é a função de soma do módulo 'operator', usada em reduções (reduceByKey(add))\n",
        "# para somar contagens por chave de forma legível e eficiente.\n",
        "from operator import add\n",
        "\n",
        "# Cria uma configuração de Spark:\n",
        "# - SparkConf(): construtor do objeto de configuração do Spark.\n",
        "# - .setMaster(\"local\"): executa localmente com 1 thread (útil p/ testes). DICA: \"local[*]\" usa todos os cores.\n",
        "# - .setAppName(\"Exercicio Nasa Logs\"): nome que aparece na UI do Spark e nos logs.\n",
        "# - .set(\"spark.executor.memory\", \"5g\"): memória destinada a cada executor (~5 GiB).\n",
        "#   Em modo \"local\", você terá 1 executor; em cluster, isso vale por executor.\n",
        "configuracao = (SparkConf()\n",
        "                .setMaster(\"local\")\n",
        "                .setAppName(\"Exercicio Nasa Logs\")\n",
        "                .set(\"spark.executor.memory\", \"5g\"))\n",
        "\n",
        "# 'type(obj)': função nativa do Python que retorna o tipo do objeto, útil para checagens rápidas.\n",
        "type(configuracao)\n",
        "\n",
        "# Cria o SparkContext com a configuração acima.\n",
        "# IMPORTANTE: deve haver apenas UM SparkContext por aplicação.\n",
        "sc = SparkContext(conf = configuracao)\n",
        "\n",
        "# Confirma que 'sc' é um SparkContext.\n",
        "type(sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "dMrLAtY7O7bF",
        "outputId": "f24706b7-7cc3-431e-f0f6-4916228054ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.context.SparkContext"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.context.SparkContext</b><br/>def __init__(master: Optional[str]=None, appName: Optional[str]=None, sparkHome: Optional[str]=None, pyFiles: Optional[List[str]]=None, environment: Optional[Dict[str, Any]]=None, batchSize: int=0, serializer: &#x27;Serializer&#x27;=CPickleSerializer(), conf: Optional[SparkConf]=None, gateway: Optional[JavaGateway]=None, jsc: Optional[JavaObject]=None, profiler_cls: Type[BasicProfiler]=BasicProfiler, udf_profiler_cls: Type[UDFBasicProfiler]=UDFBasicProfiler, memory_profiler_cls: Type[MemoryProfiler]=MemoryProfiler)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/context.py</a>Main entry point for Spark functionality. A SparkContext represents the\n",
              "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
              "broadcast variables on that cluster.\n",
              "\n",
              "When you create a new SparkContext, at least the master and app name should\n",
              "be set, either through the named parameters here or through `conf`.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "master : str, optional\n",
              "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
              "appName : str, optional\n",
              "    A name for your job, to display on the cluster web UI.\n",
              "sparkHome : str, optional\n",
              "    Location where Spark is installed on cluster nodes.\n",
              "pyFiles : list, optional\n",
              "    Collection of .zip or .py files to send to the cluster\n",
              "    and add to PYTHONPATH.  These can be paths on the local file\n",
              "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
              "environment : dict, optional\n",
              "    A dictionary of environment variables to set on\n",
              "    worker nodes.\n",
              "batchSize : int, optional, default 0\n",
              "    The number of Python objects represented as a single\n",
              "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
              "    the batch size based on object sizes, or -1 to use an unlimited\n",
              "    batch size\n",
              "serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
              "    The serializer for RDDs.\n",
              "conf : :class:`SparkConf`, optional\n",
              "    An object setting Spark properties.\n",
              "gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
              "    Use an existing gateway and JVM, otherwise a new JVM\n",
              "    will be instantiated. This is only used internally.\n",
              "jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
              "    The JavaSparkContext instance. This is only used internally.\n",
              "profiler_cls : type, optional, default :class:`BasicProfiler`\n",
              "    A class of custom Profiler used to do profiling\n",
              "udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
              "    A class of custom Profiler used to do udf profiling\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
              "the active :class:`SparkContext` before creating a new one.\n",
              "\n",
              ":class:`SparkContext` instance is not supported to share across multiple\n",
              "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
              "Use threads instead for concurrent processing purpose.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.context import SparkContext\n",
              "&gt;&gt;&gt; sc = SparkContext(&#x27;local&#x27;, &#x27;test&#x27;)\n",
              "&gt;&gt;&gt; sc2 = SparkContext(&#x27;local&#x27;, &#x27;test2&#x27;) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "ValueError: ...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 92);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leitura dos arquivos com RDD e cache"
      ],
      "metadata": {
        "id": "cgDTMdk1O9vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 4 ==========================\n",
        "# Lê os arquivos como RDDs de strings (cada elemento = 1 linha do arquivo).\n",
        "# 'sc.textFile(path)' é LAZY: a leitura real ocorre apenas quando executamos uma ACTION.\n",
        "julho = sc.textFile('aulapython/NASA_access_log_Jul95')\n",
        "agosto = sc.textFile('aulapython/NASA_access_log_Aug95')\n",
        "\n",
        "# 'cache()': mantém o RDD em memória após a primeira computação.\n",
        "# Isso evita recomputar a cadeia de transformações em AÇÕES subsequentes,\n",
        "# acelerando o workflow.\n",
        "julho = julho.cache()\n",
        "agosto = agosto.cache()\n",
        "\n",
        "# Inspeção via shell das primeiras linhas do arquivo de julho (não é Spark).\n",
        "! head -n3 aulapython/NASA_access_log_Jul95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDmYKO1tO-KT",
        "outputId": "11f3dbcd-8619-4123-90cf-31be54bec545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
            "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
            "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explorando a estrutura de uma linha de log"
      ],
      "metadata": {
        "id": "a6Ah15GePA3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 5 ==========================\n",
        "# Exemplo de linha de log (string). Isso facilita explicar parsing e tokens.\n",
        "exemplo = '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245'\n",
        "\n",
        "# Checa o tipo do objeto (deve ser 'str').\n",
        "type(exemplo)\n",
        "\n",
        "# 'split()' (sem argumentos) separa por QUALQUER espaço em branco.\n",
        "operacao = exemplo.split()\n",
        "\n",
        "print(operacao)   # Lista de tokens da linha\n",
        "print(operacao[0])# 'operacao[0]' usa INDEXAÇÃO por '[]' para obter o 1º item (host/IP)\n",
        "\n",
        "# Mapeia as primeiras 5 linhas do RDD 'julho' para o 1º token (host) e coleta para inspeção.\n",
        "julho.map(lambda line: line.split()[0]).take(5)\n",
        "\n",
        "# Variante com separador explícito: split(' ') (equivalente neste caso).\n",
        "# 'distinct()' remove duplicatas via shuffle.\n",
        "# 'count()' é ACTION: dispara a execução e retorna a contagem de elementos (hosts únicos).\n",
        "julho.map(lambda line: line.split(' ')[0]).distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92RYwYacPBin",
        "outputId": "e70580ff-ff91-4bea-e62b-a0b930238296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['199.72.81.55', '-', '-', '[01/Jul/1995:00:00:01', '-0400]', '\"GET', '/history/apollo/', 'HTTP/1.0\"', '200', '6245']\n",
            "199.72.81.55\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81983"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sobre lambda e []:\n",
        "\n",
        "lambda line: ... cria uma função anônima que recebe line (cada linha do RDD) e retorna algo.\n",
        "\n",
        "line.split(' ')[0] usa indexação com colchetes para pegar o primeiro elemento da lista de tokens."
      ],
      "metadata": {
        "id": "RVhtFj34PEN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função utilitária para hosts distintos"
      ],
      "metadata": {
        "id": "izwHoD8QPCP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 6 ==========================\n",
        "def obterQtdHosts(rdd):\n",
        "  \"\"\"\n",
        "  Retorna a quantidade de hosts distintos em um RDD de linhas de log.\n",
        "  Etapas:\n",
        "    1) line.split(' ')[0] -> extrai o host/IP (1º token).\n",
        "    2) distinct() -> remove host repetido.\n",
        "    3) count() -> ACTION que retorna a quantidade de elementos únicos.\n",
        "  \"\"\"\n",
        "  qtdHosts = rdd.map(lambda line: line.split(' ')[0]).distinct().count()\n",
        "  return qtdHosts\n",
        "\n",
        "# Número de hosts distintos em Julho (calculado diretamente)\n",
        "contagem_julho = julho.map(lambda line: line.split(' ')[0]).distinct().count()\n",
        "print(\"Numero de hosts distintos no mes de Julho:\", contagem_julho)\n",
        "\n",
        "# Usando a função\n",
        "obterQtdHosts(julho)\n",
        "\n",
        "# Número de hosts distintos em Agosto\n",
        "contagem_agosto = agosto.map(lambda line: line.split(' ')[0]).distinct().count()\n",
        "print(\"Numero de hosts distintos no mes de Agosto:\", contagem_agosto)\n",
        "\n",
        "# Usando a função\n",
        "obterQtdHosts(agosto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzjoUyDcPG2V",
        "outputId": "a8a6c268-a11d-4bdc-ac21-2aeb5da4f713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de hosts distintos no mes de Julho: 81983\n",
            "Numero de hosts distintos no mes de Agosto: 75060\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75060"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função para detectar linhas com código HTTP 404"
      ],
      "metadata": {
        "id": "7VQSLb2KPIH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 7 ==========================\n",
        "def codigo404(linha):\n",
        "    \"\"\"\n",
        "    Retorna True se o penúltimo token da linha for '404' (código HTTP de \"Not Found\").\n",
        "    Estrutura típica de linha:\n",
        "      ... \"REQUEST\" STATUS BYTES\n",
        "      penúltimo token = STATUS\n",
        "      último token    = BYTES (pode ser '-')\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 'linha.split()[-2]' pega o penúltimo token usando índice negativo.\n",
        "        # Em Python, índices negativos contam a partir do fim: [-1] último, [-2] penúltimo.\n",
        "        codigohttp = linha.split()[-2]\n",
        "        if codigohttp == '404':\n",
        "            return True\n",
        "    except:\n",
        "        # Linhas malformadas (sem tokens suficientes) ou erros de parsing caem aqui.\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "# Teste da função (esta linha tem status 200, então deve retornar False)\n",
        "codigo404(exemplo)\n",
        "\n",
        "# Apenas para relembrar o conteúdo\n",
        "exemplo\n",
        "\n",
        "print(exemplo)\n",
        "\n",
        "# Linha sintética com status 404 (para validar True)\n",
        "exemplo2 = '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 404 6245'\n",
        "codigo404(exemplo2)  # Esperado: True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MMxe7TvPJXc",
        "outputId": "08c2eece-ff73-496d-eeb0-352cd6ae4665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtrando os 404 por mês e contando"
      ],
      "metadata": {
        "id": "xgKADuiQPKzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 8 ==========================\n",
        "# Filtra linhas de julho que têm status 404 usando a função robusta 'codigo404'.\n",
        "# 'cache()' para reutilizar os resultados sem recomputar o filtro.\n",
        "erros404_julho = julho.filter(codigo404).cache()\n",
        "\n",
        "# Em agosto usamos uma lambda equivalente, mas MENOS robusta (assume que sempre existe penúltimo token).\n",
        "# Preferir a versão com try/except para tolerar linhas malformadas.\n",
        "erros404_agosto = agosto.filter(lambda linha: linha.split(' ')[-2] == '404').cache()\n",
        "\n",
        "# 'count()' é ACTION que dispara a execução e retorna a quantidade de elementos no RDD.\n",
        "print('Erros 404 em Julho: %s' % erros404_julho.count())\n",
        "print('Erros 404 em Agosto: %s' % erros404_agosto.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE-MT-0jPMVL",
        "outputId": "19ed462a-7e41-49b4-eaff-220d9a15d0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erros 404 em Julho: 10845\n",
            "Erros 404 em Agosto: 10056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extração de URL dentro do bloco \"REQUEST\""
      ],
      "metadata": {
        "id": "DxYmW5XxPOJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 9 ==========================\n",
        "print(exemplo)\n",
        "print(exemplo.split('\"'))                 # Quebra a linha nas aspas -> [\"antes\", 'GET /... HTTP/1.x', \"depois\"]\n",
        "print(exemplo.split('\"')[1].split(' ')[1])# Pega o bloco do meio (índice 1) e, dele, o 2º token => a URL\n",
        "\n",
        "# Extrai apenas a URL das linhas 404 de julho (amostra de 5)\n",
        "erros404_julho.map(lambda linha: linha.split('\"')[1].split(' ')[1]).take(5)\n",
        "\n",
        "# Mapeia cada URL para o par (url, 1), ou seja, 1 ocorrência por linha/URL. Amostra.\n",
        "erros404_julho.map(lambda linha: linha.split('\"')[1].split(' ')[1]) \\\n",
        "              .map(lambda urls: (urls, 1)) \\\n",
        "              .take(5)\n",
        "\n",
        "# Pipeline completo: URLs -> (url, 1) -> contagem por chave com reduceByKey(add)\n",
        "urls = erros404_julho.map(lambda linha: linha.split('\"')[1].split(' ')[1])\n",
        "counts = urls.map(lambda urls: (urls, 1)).reduceByKey(add)\n",
        "\n",
        "counts.take(10)  # amostra de 10 pares (url, contagem)\n",
        "type(counts)     # RDD[Tuple[str, int]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "ZfnICl-4POY9",
        "outputId": "63c47aa6-f7ed-46d6-e47b-b04ebb3bfbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
            "['199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ', 'GET /history/apollo/ HTTP/1.0', ' 200 6245']\n",
            "/history/apollo/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.rdd.PipelinedRDD</b><br/>def __init__(prev: RDD[T], func: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool=False, isFromBarrier: bool=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py</a>Examples\n",
              "--------\n",
              "Pipelined maps:\n",
              "\n",
              "&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4])\n",
              "&gt;&gt;&gt; rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()\n",
              "[4, 8, 12, 16]\n",
              "&gt;&gt;&gt; rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()\n",
              "[4, 8, 12, 16]\n",
              "\n",
              "Pipelined reduces:\n",
              "\n",
              "&gt;&gt;&gt; from operator import add\n",
              "&gt;&gt;&gt; rdd.map(lambda x: 2 * x).reduce(add)\n",
              "20\n",
              "&gt;&gt;&gt; rdd.flatMap(lambda x: [x, x]).reduce(add)\n",
              "20</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 5395);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por que reduceByKey(add)?\n",
        "\n",
        "Ele faz combiner local (agregação por partição) ANTES de embaralhar os dados (shuffle), sendo muito mais eficiente que groupByKey()."
      ],
      "metadata": {
        "id": "EPQyySl-PQ1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 5 URLs que mais geram 404"
      ],
      "metadata": {
        "id": "ML9TcNiMPRPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 10 ==========================\n",
        "def top5_hosts404(rdd):\n",
        "    \"\"\"\n",
        "    Calcula as 5 URLs com mais erros 404 em um RDD de linhas de log 404.\n",
        "    Passos:\n",
        "      1) extrair URL: linha.split('\"')[1].split(' ')[1]\n",
        "      2) map para (url, 1)\n",
        "      3) reduceByKey(add) para somar\n",
        "      4) sortBy(lambda par: -par[1]) para ordenar por contagem desc\n",
        "      5) take(5)\n",
        "    \"\"\"\n",
        "    urls = rdd.map(lambda linha: linha.split('\"')[1].split(' ')[1])   # '/history/apollo/'\n",
        "    counts = urls.map(lambda urls: (urls, 1)).reduceByKey(add)        # (url, soma)\n",
        "    top5 = counts.sortBy(lambda par: -par[1]).take(5)                 # ordena por -contagem (descendente)\n",
        "    return top5\n",
        "\n",
        "# Retorna lista com 5 tuplas (url, contagem), ordenadas por maior contagem\n",
        "top5_hosts404(erros404_julho)\n",
        "top5_hosts404(erros404_agosto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQCQ6hsPSe2",
        "outputId": "b894e1d4-23f8-4ddd-ccdd-51556c2d3dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('/pub/winvn/readme.txt', 1337),\n",
              " ('/pub/winvn/release.txt', 1185),\n",
              " ('/shuttle/missions/STS-69/mission-STS-69.html', 683),\n",
              " ('/images/nasa-logo.gif', 319),\n",
              " ('/shuttle/missions/sts-68/ksc-upclose.gif', 253)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraindo data e contando 404 por dia"
      ],
      "metadata": {
        "id": "AZtaYHdpPUe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 11 ==========================\n",
        "print(exemplo)                      # relembrando o formato geral\n",
        "print(exemplo.split('[')[1])        # pega tudo após o primeiro '[': \"01/Jul/1995:00:00:01 -0400]\"\n",
        "print(exemplo.split('[')[1].split(':')[0])  # fica só a parte \"01/Jul/1995\"\n",
        "\n",
        "# Função para contar erros 404 por DIA (retorna lista Python de tuplas (dia, contagem)).\n",
        "def contador_dias_404(rdd):\n",
        "    \"\"\"\n",
        "    1) Extrai o dia no formato 'dd/Mon/yyyy' do trecho entre colchetes.\n",
        "    2) Mapeia para (dia, 1) e soma com reduceByKey(add).\n",
        "    3) collect() traz o resultado (lista de tuplas) para o Driver.\n",
        "    \"\"\"\n",
        "    dias = rdd.map(lambda linha: linha.split('[')[1].split(':')[0])  # '01/Jul/1995'\n",
        "    counts = dias.map(lambda dia: (dia, 1)).reduceByKey(add).collect()\n",
        "    return counts\n",
        "\n",
        "print(contador_dias_404(erros404_julho))  # lista Python de (\"01/Jul/1995\", 123), ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDJ7iZN-PUvx",
        "outputId": "745ddb82-528b-4897-f04d-91dad238d6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
            "01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
            "01/Jul/1995\n",
            "[('01/Jul/1995', 316), ('09/Jul/1995', 348), ('21/Jul/1995', 334), ('26/Jul/1995', 336), ('27/Jul/1995', 336), ('17/Jul/1995', 406), ('20/Jul/1995', 428), ('23/Jul/1995', 233), ('11/Jul/1995', 471), ('12/Jul/1995', 471), ('03/Jul/1995', 474), ('05/Jul/1995', 497), ('07/Jul/1995', 570), ('22/Jul/1995', 192), ('28/Jul/1995', 94), ('02/Jul/1995', 291), ('13/Jul/1995', 532), ('19/Jul/1995', 639), ('14/Jul/1995', 413), ('16/Jul/1995', 257), ('24/Jul/1995', 328), ('04/Jul/1995', 359), ('06/Jul/1995', 640), ('08/Jul/1995', 302), ('10/Jul/1995', 398), ('15/Jul/1995', 254), ('18/Jul/1995', 465), ('25/Jul/1995', 461)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordenando os resultados (Python puro)"
      ],
      "metadata": {
        "id": "W3D_QOtoPV65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 12 ==========================\n",
        "# Exemplo bobo de ordenação alfabética com Python puro (sem Spark).\n",
        "a = (\"b\", \"g\", \"a\", \"d\", \"f\", \"c\", \"h\", \"e\")\n",
        "x = sorted(a)  # 'sorted' retorna NOVA lista ordenada; não altera 'a'.\n",
        "print(x)\n",
        "\n",
        "# Agora, ordenando os dias por quantidade de 404 (descendente) com Python puro:\n",
        "# 'contador_dias_404(...)' já devolve uma LISTA Python de tuplas (dia, contagem).\n",
        "# 'sorted(lista, key=..., reverse=...)' ordena essa lista.\n",
        "#\n",
        "# key=lambda x: -x[1]\n",
        "# - 'lambda x: ...' cria uma função anônima.\n",
        "# - 'x' é cada tupla (dia, contagem).\n",
        "# - 'x[1]' é a CONTAGEM (2º elemento da tupla) — indexação via '[]'.\n",
        "# - '-x[1]' usa o NEGATIVO da contagem para transformar a ordenação padrão (asc) em desc.\n",
        "#\n",
        "# Equivalente (mais legível): key=lambda x: x[1], reverse=True\n",
        "\n",
        "sorted(contador_dias_404(erros404_julho), key=lambda x: -x[1])\n",
        "\n",
        "# Ordenação por TEXTO (dia), desc: para strings, use reverse=True (não faz sentido negar string).\n",
        "sorted(contador_dias_404(erros404_julho), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# Ordenação por TEXTO (dia), asc:\n",
        "sorted(contador_dias_404(erros404_julho), key=lambda x: x[0])\n",
        "\n",
        "# Reaproveita a função para agosto (lista não ordenada):\n",
        "contador_dias_404(erros404_agosto)\n",
        "\n",
        "# Conversão de string para inteiro (exemplo auxiliar):\n",
        "int('10')  # retorna 10 (int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5u6cHPTPYJc",
        "outputId": "ba8af950-49ad-4d1b-e5e7-bf1f3801ab7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soma total de bytes transferidos"
      ],
      "metadata": {
        "id": "njBioqLiPaS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 13 ==========================\n",
        "def quantidade_bytes_acumulados(rdd):\n",
        "    \"\"\"\n",
        "    Soma o campo de BYTES (último token) de cada linha.\n",
        "    O campo pode ser '-' (sem valor) -> tratamos como 0.\n",
        "    Também ignoramos valores inválidos via try/except.\n",
        "    \"\"\"\n",
        "    def contador(linha):\n",
        "        try:\n",
        "            # 'linha.split(\" \")[-1]' pega o ÚLTIMO token (índice -1) — BYTES.\n",
        "            # int(...) converte de string para inteiro; se for '-', cai no except e volta 0.\n",
        "            count = int(linha.split(\" \")[-1])\n",
        "            if count < 0:\n",
        "                # bytes negativos não fazem sentido — dispara exceção para tratar como 0.\n",
        "                raise ValueError()\n",
        "            return count\n",
        "        except:\n",
        "            # Retorna 0 para linhas malformadas ou com '-' em BYTES.\n",
        "            return 0\n",
        "\n",
        "    # 'map(contador)': aplica a função acima em cada linha, produzindo um RDD de inteiros (bytes).\n",
        "    # 'reduce(add)': soma todos os inteiros de forma DISTRIBUÍDA (reduceByKey é para pares (k,v); aqui é reduce simples).\n",
        "    count = rdd.map(contador).reduce(add)\n",
        "    return count\n",
        "\n",
        "print('Quantidade de bytes total em Julho: %s' % quantidade_bytes_acumulados(julho))\n",
        "print('Quantidade de bytes total em Agosto: %s' % quantidade_bytes_acumulados(agosto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUI9en86PafQ",
        "outputId": "d0935b4e-5cee-4438-c6d4-89e9b11e7585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de bytes total em Julho: 38695973491\n",
            "Quantidade de bytes total em Agosto: 26828341424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encerrando o SparkContext"
      ],
      "metadata": {
        "id": "uYJoZ7sFPcrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== CÉLULA 14 ==========================\n",
        "# Encerra o contexto do Spark e libera recursos locais/cluster.\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "_YEZIuxrPc6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}